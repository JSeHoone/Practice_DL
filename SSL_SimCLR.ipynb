{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Supervised Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generative Learning \n",
    "2. Proxy Task Learning \n",
    "3. Contrative Learning * 주된 방법론 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 출처 : https://jimmy-ai.tistory.com/312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Contrative Learning \n",
    "'''\n",
    "Contrative Loss를 사용하는데 중요한 전제가 있다.\n",
    "Promise: Augmentation은 Segmentation한 정보를 바꾸지 않는다.\n",
    "\n",
    "\n",
    "같은 이미지에서 Augmentation된 이미지 집단을 Positive Sample\n",
    "다른 이미지에서 Augmentation된 이미지 집단을 Negative Sample\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s\\anaconda3\\envs\\pytorch_test\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "## Data 불러오기\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000, 1, 28, 28])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "## Data split (60000 : 10000)\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_x =torch.Tensor(np.array(mnist['data'])).float().reshape(-1,1,28,28)[:60000]\n",
    "train_y = torch.Tensor(np.array(list(map(np.int_, mnist.target))))[:60000]\n",
    "\n",
    "test_x = torch.tensor(np.array(mnist.data)).float().reshape(-1, 1, 28, 28)[60000:]\n",
    "test_y = torch.Tensor(np.array(list(map(np.int_, mnist.target))))[60000:]\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutout_and_rotate(image):\n",
    "    image = image.clone().detach() # 얕은 복사 문제 주의(원본 유지)\n",
    "    x_start = np.random.randint(20) # cut out 시작할 x축 위치(0~19 중 1개)\n",
    "    y_start = np.random.randint(20) # cut out 시작할 y축 위치(0~19 중 1개)\n",
    "\n",
    "    image[..., x_start:x_start+9, y_start:y_start+9] = 255 / 2 # 해당 부분 회색 마킹\n",
    "    return torch.rot90(image, 1, [-2, -1]) # 마지막 두 axis 기준 90도 회전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import style\n",
    "\n",
    "# 흰색 배경 및 크기 지정\n",
    "style.use('default')\n",
    "figure = plt.figure()\n",
    "figure.set_size_inches(4, 2)\n",
    "\n",
    "# 흑백으로 출력하기 위한 스타일 설정\n",
    "style.use('grayscale')\n",
    "\n",
    "# 1 * 2 사이즈의 격자 설정\n",
    "axes = []\n",
    "for i in range(1, 3):\n",
    "    axes.append(figure.add_subplot(1, 2, i))\n",
    "\n",
    "# 첫 이미지에 대한 원본 이미지 및 augmentation 수행된 이미지 시각화\n",
    "img_example = train_x[0].clone().detach().cpu()\n",
    "original = np.array(img_example).reshape(-1, 28).astype(int)\n",
    "aug_img = np.array(cutout_and_rotate(img_example)).reshape(-1, 28).astype(int)\n",
    "\n",
    "axes[0].matshow(original)\n",
    "axes[1].matshow(aug_img)\n",
    "\n",
    "# 제목 설정 및 눈금 제거\n",
    "axes[0].set_axis_off()\n",
    "axes[0].set_title('original')\n",
    "axes[1].set_axis_off() \n",
    "axes[1].set_title('augmentation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True,  True,  True,  True],\n",
      "        [ True, False,  True,  True,  True],\n",
      "        [ True,  True, False,  True,  True],\n",
      "        [ True,  True,  True, False,  True],\n",
      "        [ True,  True,  True,  True, False]])\n",
      "tensor([False, False, False, False, False])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.ones((5, 5), dtype=bool)\n",
    "mask = mask.fill_diagonal_(0) # 대각선의 값은 0으로 대체 한다는 의미\n",
    "print(mask)\n",
    "print(mask.diag())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, stride=1)\n",
    "        self.fc = nn.Linear(4 * 4 * 20, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) # (batch, 1, 28, 28) -> (batch, 10, 24, 24)\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) # (batch, 10, 24, 24) -> (batch, 10, 12, 12)\n",
    "\n",
    "        x = F.relu(self.conv2(x)) # (batch, 10, 12, 12) -> (batch, 20, 8, 8)\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) # (batch, 20, 8, 8) -> (batch, 20, 4, 4)\n",
    "\n",
    "        x = x.view(-1, 4 * 4 * 20) # (batch, 20, 4, 4) -> (batch, 320)\n",
    "\n",
    "        x = F.relu(self.fc(x)) # (batch, 320) -> (batch, 100)\n",
    "        return x # (batch, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True,  True, False,  True,  True],\n",
      "        [ True, False,  True,  True, False,  True],\n",
      "        [ True,  True, False,  True,  True, False],\n",
      "        [False,  True,  True, False,  True,  True],\n",
      "        [ True, False,  True,  True, False,  True],\n",
      "        [ True,  True, False,  True,  True, False]])\n"
     ]
    }
   ],
   "source": [
    "## batch size == 3\n",
    "N = 3 * 2\n",
    "mask = torch.ones((N,N), dtype=bool)\n",
    "mask = mask.fill_diagonal_(0)\n",
    "\n",
    "for i in range(3):\n",
    "    mask[i,3 + i] = 0\n",
    "    mask[3 + i, i] = 0\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1.,   2.,   3.,   4.,   5.],\n",
      "        [ -1.,  -2.,  -3.,  -4.,  -5.],\n",
      "        [  6.,   7.,   8.,   9.,  10.],\n",
      "        [ -6.,  -7.,  -8.,  -9., -10.]])\n",
      "==========\n",
      "tensor([[[  1.,   2.,   3.,   4.,   5.],\n",
      "         [ -1.,  -2.,  -3.,  -4.,  -5.],\n",
      "         [  6.,   7.,   8.,   9.,  10.],\n",
      "         [ -6.,  -7.,  -8.,  -9., -10.]]]) torch.Size([1, 4, 5])\n",
      "tensor([[[  1.,   2.,   3.,   4.,   5.]],\n",
      "\n",
      "        [[ -1.,  -2.,  -3.,  -4.,  -5.]],\n",
      "\n",
      "        [[  6.,   7.,   8.,   9.,  10.]],\n",
      "\n",
      "        [[ -6.,  -7.,  -8.,  -9., -10.]]]) torch.Size([4, 1, 5])\n",
      "==========\n",
      "tensor([[ 1.0000, -1.0000,  0.9650, -0.9650],\n",
      "        [-1.0000,  1.0000, -0.9650,  0.9650],\n",
      "        [ 0.9650, -0.9650,  1.0000, -1.0000],\n",
      "        [-0.9650,  0.9650, -1.0000,  1.0000]])\n",
      "==========\n",
      "tensor([0.9650, 0.9650])\n",
      "tensor([0.9650, 0.9650])\n"
     ]
    }
   ],
   "source": [
    "## 코드를 이해하기 위한 코드들 \n",
    "z_i = [[1,2,3,4,5],[-1,-2,-3,-4,-5]]\n",
    "z_j = [[6,7,8,9,10],[-6,-7,-8,-9,-10]]\n",
    "z_i = torch.Tensor(z_i)\n",
    "z_j = torch.Tensor(z_j)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "emb = torch.cat([z_i,z_j], dim = 0)\n",
    "print(emb)\n",
    "print('='*10)\n",
    "\n",
    "print(emb.unsqueeze(0), emb.unsqueeze(0).shape) # unsqueeze는 차원을 생성해주는 함수 dim=0인 차원을 생성\n",
    "print(emb.unsqueeze(1), emb.unsqueeze(1).shape) # dim =1 인 차원을 생성해주는 것 \n",
    "\n",
    "\n",
    "print('='*10)\n",
    "similarity_f = nn.CosineSimilarity(dim=2)\n",
    "sim = similarity_f(emb.unsqueeze(0), emb.unsqueeze(1)) \n",
    "print(sim)\n",
    "\n",
    "sim_i_j = torch.diag(sim, 2)\n",
    "sim_j_i = torch.diag(sim, -2)\n",
    "\n",
    "print('=' * 10)\n",
    "print(sim_i_j)\n",
    "print(sim_j_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimCLR (contrative learning)\n",
    "# 출처 : https://medium.com/the-owl/simclr-in-pytorch-5f290cb11dd7\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimCLR_Loss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.mask = self.mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    # loss 분모 부분의 negative sample 간의 내적 합만을 가져오기 위한 마스킹 행렬\n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size # batch size를 왜 곱하기 2 해주는 걸까?\n",
    "        mask = torch.ones((N, N), dtype=bool) # batch size의 두배를 만들어 mask를 만들어줌\n",
    "        mask = mask.fill_diagonal_(0) # 대각선의 값을 0으로 변환 (mask를 만들어 주고 대각선은 0으로 만들어줌)\n",
    "        \n",
    "        ## mask를 왜 이렇게 만들었을까?.. \n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "        return mask\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        # 2개의 이미지 embedding이 concat이 되니 *2를 해준 것이다 !\n",
    "        N = 2 * self.batch_size \n",
    "\n",
    "        z = torch.cat((z_i, z_j), dim=0) # concatenate (Tensor의 리스트를 묶어 주는 함수임) (행으로 합쳐줌) 열로 합치는 것은 dim=1\n",
    "        ## Original vetor랑 Augmentation vector를 concat해줌\n",
    "\n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "        ## Cosine 유사도를 계산하기 위해서 unsqueeze로 해줌 -> temperature를 나눠주는 이유는 뭘까? \n",
    "\n",
    "        # loss 분자 부분의 원본 - augmentation 이미지 간의 내적 합을 가져오기 위한 부분\n",
    "        sim_i_j = torch.diag(sim, self.batch_size)\n",
    "        sim_j_i = torch.diag(sim, -self.batch_size)\n",
    "        \n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_samples = sim[self.mask].reshape(N, -1)\n",
    "        \n",
    "        labels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long()\n",
    "        \n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(next(iter(dataloader))[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(dataloader))[0].shape)\n",
    "print(next(iter(dataloader))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training \n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "X_train_aug = cutout_and_rotate(train_x) # 각 X_train 데이터에 대하여 augmentation\n",
    "X_train_aug = X_train_aug.to(device) # 학습을 위하여 GPU에 선언\n",
    "\n",
    "dataset = TensorDataset(train_x, X_train_aug) # augmentation된 데이터와 pair\n",
    "batch_size = 32\n",
    "\n",
    "dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size = batch_size)\n",
    "\n",
    "model = CNN() # 모델 변수 선언\n",
    "loss_func = SimCLR_Loss(batch_size, temperature = 0.5) # loss 함수 선언\n",
    "\n",
    "# train 코드 예시\n",
    "epochs = 10\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for i in range(1, epochs + 1):\n",
    "    total_loss = 0\n",
    "    for data in tqdm(dataloader):\n",
    "        origin_vec = model(data[0])\n",
    "        aug_vec = model(data[1])\n",
    "\n",
    "        loss = loss_func(origin_vec, aug_vec)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch : %d, Avg Loss : %.4f'%(i, total_loss / len(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downstream classification\n",
    "\n",
    "class CNN_classifier(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.CNN = model # contrastive learning으로 학습해둔 모델을 불러오기\n",
    "        self.mlp = nn.Linear(100, 10) # class 차원 개수로 projection\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.CNN(x) # (batch, 100)으로 변환\n",
    "        x = self.mlp(x) # (batch, 10)으로 변환\n",
    "        return x # (batch, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dataset = TensorDataset(X_train, y_train) # 데이터와 라벨 간의 pair\n",
    "batch_size = 32\n",
    "\n",
    "class_dataloader = DataLoader(\n",
    "            class_dataset,\n",
    "            batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_classifier(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.CNN = model # contrastive learning으로 학습해둔 모델을 불러오기\n",
    "        self.mlp = nn.Linear(100, 10) # class 차원 개수로 projection\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.CNN(x) # (batch, 100)으로 변환\n",
    "        x = self.mlp(x) # (batch, 10)으로 변환\n",
    "        return x # (batch, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\beebe\\Desktop\\git_practice\\Practice_DL\\SSL_SimCLR.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/beebe/Desktop/git_practice/Practice_DL/SSL_SimCLR.ipynb#ch0000015?line=0'>1</a>\u001b[0m classifier \u001b[39m=\u001b[39m CNN_classifier(model)\u001b[39m.\u001b[39mto(device) \u001b[39m# 모델 선언, GPU 활용 지정\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/beebe/Desktop/git_practice/Practice_DL/SSL_SimCLR.ipynb#ch0000015?line=2'>3</a>\u001b[0m classifier_loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss() \u001b[39m# 분류를 위한 loss 함수\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/beebe/Desktop/git_practice/Practice_DL/SSL_SimCLR.ipynb#ch0000015?line=4'>5</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = CNN_classifier(model).to(device) # 모델 선언, GPU 활용 지정\n",
    "\n",
    "classifier_loss = nn.CrossEntropyLoss() # 분류를 위한 loss 함수\n",
    "\n",
    "epochs = 10\n",
    "classifier.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n",
    "\n",
    "for i in range(1, epochs + 1):\n",
    "    correct = 0\n",
    "    for data in tqdm(class_dataloader):\n",
    "        logits = classifier(data[0])\n",
    "\n",
    "        loss = classifier_loss(logits, data[1].long())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct += torch.sum(torch.argmax(logits, 1) == data[1]).item() # 정확도 산출을 위하여 정답 개수 누적\n",
    "\n",
    "    print('Epoch : %d, Train Accuracy : %.2f%%'%(i, correct * 100 / len(train_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(test_x, test_y) # 테스트 데이터와 라벨 pair\n",
    "batch_size = 32\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size = batch_size)\n",
    "\n",
    "classifier.eval() # 테스트 모드로 전환\n",
    "\n",
    "correct = 0\n",
    "for data in tqdm(test_dataloader):\n",
    "\n",
    "    logits = classifier(data[0])\n",
    "    correct += torch.sum(torch.argmax(logits, 1) == data[1]).item() # 정확도 산출을 위하여 정답 개수 누적\n",
    "\n",
    "print('Test Accuracy : %.2f%%'%(correct * 100 / len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4cbe91950c8d3c47ce6c9e2bb78622c27789d701432f2f96e459ce65d9fe504"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fcpractice1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
