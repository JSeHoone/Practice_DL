{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generative Learning \n",
    "2. Proxy Task Learning \n",
    "3. Contrative Learning * 주된 방법론 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 출처 : https://jimmy-ai.tistory.com/312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Contrative Learning \n",
    "'''\n",
    "Contrative Loss를 사용하는데 중요한 전제가 있다.\n",
    "Promise: Augmentation은 Segmentation한 정보를 바꾸지 않는다.\n",
    "\n",
    "\n",
    "같은 이미지에서 Augmentation된 이미지 집단을 Positive Sample\n",
    "다른 이미지에서 Augmentation된 이미지 집단을 Negative Sample\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data 불러오기\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000, 1, 28, 28])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "## Data split (60000 : 10000)\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_x =torch.Tensor(np.array(mnist['data'])).float().reshape(-1,1,28,28)[:60000]\n",
    "train_y = torch.Tensor(np.array(list(map(np.int_, mnist.target))))[:60000]\n",
    "\n",
    "test_x = torch.tensor(np.array(mnist.data)).float().reshape(-1, 1, 28, 28)[60000:]\n",
    "test_y = torch.Tensor(np.array(list(map(np.int_, mnist.target))))[60000:]\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutout_and_rotate(image):\n",
    "    image = image.clone().detach() # 얕은 복사 문제 주의(원본 유지)\n",
    "    x_start = np.random.randint(20) # cut out 시작할 x축 위치(0~19 중 1개)\n",
    "    y_start = np.random.randint(20) # cut out 시작할 y축 위치(0~19 중 1개)\n",
    "\n",
    "    image[..., x_start:x_start+9, y_start:y_start+9] = 255 / 2 # 해당 부분 회색 마킹\n",
    "    return torch.rot90(image, 1, [-2, -1]) # 마지막 두 axis 기준 90도 회전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import style\n",
    "\n",
    "# 흰색 배경 및 크기 지정\n",
    "style.use('default')\n",
    "figure = plt.figure()\n",
    "figure.set_size_inches(4, 2)\n",
    "\n",
    "# 흑백으로 출력하기 위한 스타일 설정\n",
    "style.use('grayscale')\n",
    "\n",
    "# 1 * 2 사이즈의 격자 설정\n",
    "axes = []\n",
    "for i in range(1, 3):\n",
    "    axes.append(figure.add_subplot(1, 2, i))\n",
    "\n",
    "# 첫 이미지에 대한 원본 이미지 및 augmentation 수행된 이미지 시각화\n",
    "img_example = train_x[0].clone().detach().cpu()\n",
    "original = np.array(img_example).reshape(-1, 28).astype(int)\n",
    "aug_img = np.array(cutout_and_rotate(img_example)).reshape(-1, 28).astype(int)\n",
    "\n",
    "axes[0].matshow(original)\n",
    "axes[1].matshow(aug_img)\n",
    "\n",
    "# 제목 설정 및 눈금 제거\n",
    "axes[0].set_axis_off()\n",
    "axes[0].set_title('original')\n",
    "axes[1].set_axis_off() \n",
    "axes[1].set_title('augmentation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True,  True,  True,  True],\n",
      "        [ True, False,  True,  True,  True],\n",
      "        [ True,  True, False,  True,  True],\n",
      "        [ True,  True,  True, False,  True],\n",
      "        [ True,  True,  True,  True, False]])\n",
      "tensor([False, False, False, False, False])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.ones((5, 5), dtype=bool)\n",
    "mask = mask.fill_diagonal_(0) # 대각선의 값은 \n",
    "print(mask)\n",
    "print(mask.diag())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1.,   2.,   3.,   4.,   5.],\n",
      "        [ -1.,  -2.,  -3.,  -4.,  -5.],\n",
      "        [  6.,   7.,   8.,   9.,  10.],\n",
      "        [ -6.,  -7.,  -8.,  -9., -10.]])\n",
      "==========\n",
      "tensor([[[  1.,   2.,   3.,   4.,   5.],\n",
      "         [ -1.,  -2.,  -3.,  -4.,  -5.],\n",
      "         [  6.,   7.,   8.,   9.,  10.],\n",
      "         [ -6.,  -7.,  -8.,  -9., -10.]]])\n",
      "tensor([[[  1.,   2.,   3.,   4.,   5.]],\n",
      "\n",
      "        [[ -1.,  -2.,  -3.,  -4.,  -5.]],\n",
      "\n",
      "        [[  6.,   7.,   8.,   9.,  10.]],\n",
      "\n",
      "        [[ -6.,  -7.,  -8.,  -9., -10.]]])\n",
      "==========\n",
      "tensor([[ 1.0000, -1.0000,  0.9650, -0.9650],\n",
      "        [-1.0000,  1.0000, -0.9650,  0.9650],\n",
      "        [ 0.9650, -0.9650,  1.0000, -1.0000],\n",
      "        [-0.9650,  0.9650, -1.0000,  1.0000]])\n",
      "==========\n",
      "tensor([0.9650, 0.9650])\n",
      "tensor([0.9650, 0.9650])\n"
     ]
    }
   ],
   "source": [
    "z_i = [[1,2,3,4,5],[-1,-2,-3,-4,-5]]\n",
    "z_j = [[6,7,8,9,10],[-6,-7,-8,-9,-10]]\n",
    "z_i = torch.Tensor(z_i)\n",
    "z_j = torch.Tensor(z_j)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "emb = torch.cat([z_i,z_j], dim = 0)\n",
    "print(emb)\n",
    "print('='*10)\n",
    "\n",
    "print(emb.unsqueeze(0))\n",
    "print(emb.unsqueeze(1))\n",
    "\n",
    "\n",
    "print('='*10)\n",
    "similarity_f = nn.CosineSimilarity(dim=2)\n",
    "sim = similarity_f(emb.unsqueeze(0), emb.unsqueeze(1))\n",
    "print(sim)\n",
    "\n",
    "sim_i_j = torch.diag(sim, 2)\n",
    "sim_j_i = torch.diag(sim, -2)\n",
    "\n",
    "print('=' * 10)\n",
    "print(sim_i_j)\n",
    "print(sim_j_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, stride=1)\n",
    "        self.fc = nn.Linear(4 * 4 * 20, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) # (batch, 1, 28, 28) -> (batch, 10, 24, 24)\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) # (batch, 10, 24, 24) -> (batch, 10, 12, 12)\n",
    "\n",
    "        x = F.relu(self.conv2(x)) # (batch, 10, 12, 12) -> (batch, 20, 8, 8)\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2) # (batch, 20, 8, 8) -> (batch, 20, 4, 4)\n",
    "\n",
    "        x = x.view(-1, 4 * 4 * 20) # (batch, 20, 4, 4) -> (batch, 320)\n",
    "\n",
    "        x = F.relu(self.fc(x)) # (batch, 320) -> (batch, 100)\n",
    "        return x # (batch, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimCLR (contrative learning)\n",
    "# 출처 : https://medium.com/the-owl/simclr-in-pytorch-5f290cb11dd7\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimCLR_Loss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.mask = self.mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    # loss 분모 부분의 negative sample 간의 내적 합만을 가져오기 위한 마스킹 행렬\n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0) # 대각선의 값을 0으로 변환\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "        return mask\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "\n",
    "        N = 2 * self.batch_size\n",
    "\n",
    "        z = torch.cat((z_i, z_j), dim=0) # concatenate (Tensor의 리스트를 묶어 주는 함수임)\n",
    "        ## Positive sample embedding tensor와 Negative sample emnedding Tensor list를 묶어주는 이유는 뭘까???\n",
    "\n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "        ## Cosne 유사도를 계산하기 위해서 unsqueeze로 해줌\n",
    "\n",
    "        # loss 분자 부분의 원본 - augmentation 이미지 간의 내적 합을 가져오기 위한 부분\n",
    "        sim_i_j = torch.diag(sim, self.batch_size)\n",
    "        sim_j_i = torch.diag(sim, -self.batch_size)\n",
    "        \n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_samples = sim[self.mask].reshape(N, -1)\n",
    "        \n",
    "        labels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long()\n",
    "        \n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15dc78e1236e4cf1ae41a9bb49859f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Avg Loss : 2.9770\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053323f61eaa4b09badf55a5ecdf71f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2, Avg Loss : 2.8257\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dbc85cdaea43f58e593dc5061e5bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3, Avg Loss : 2.7462\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1b678066274e18a9aa689dd6e84210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4, Avg Loss : 2.7167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a57a6a2f6d149de8a393cdfc584311c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\beebe\\Desktop\\git_practice\\Practice_DL\\SSL_SimCLR.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/beebe/Desktop/git_practice/Practice_DL/SSL_SimCLR.ipynb#ch0000012?line=32'>33</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/beebe/Desktop/git_practice/Practice_DL/SSL_SimCLR.ipynb#ch0000012?line=34'>35</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/beebe/Desktop/git_practice/Practice_DL/SSL_SimCLR.ipynb#ch0000012?line=35'>36</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/beebe/Desktop/git_practice/Practice_DL/SSL_SimCLR.ipynb#ch0000012?line=36'>37</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/beebe/Desktop/git_practice/Practice_DL/SSL_SimCLR.ipynb#ch0000012?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch : \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, Avg Loss : \u001b[39m\u001b[39m%.4f\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m(i, total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataloader)))\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\fcpractice1\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/beebe/miniforge3/envs/fcpractice1/lib/site-packages/torch/_tensor.py?line=386'>387</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/beebe/miniforge3/envs/fcpractice1/lib/site-packages/torch/_tensor.py?line=387'>388</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/beebe/miniforge3/envs/fcpractice1/lib/site-packages/torch/_tensor.py?line=388'>389</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/beebe/miniforge3/envs/fcpractice1/lib/site-packages/torch/_tensor.py?line=389'>390</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/beebe/miniforge3/envs/fcpractice1/lib/site-packages/torch/_tensor.py?line=393'>394</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/beebe/miniforge3/envs/fcpractice1/lib/site-packages/torch/_tensor.py?line=394'>395</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/beebe/miniforge3/envs/fcpractice1/lib/site-packages/torch/_tensor.py?line=395'>396</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\fcpractice1\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/beebe/miniforge3/envs/fcpractice1/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/beebe/miniforge3/envs/fcpractice1/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/beebe/miniforge3/envs/fcpractice1/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/beebe/miniforge3/envs/fcpractice1/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/beebe/miniforge3/envs/fcpractice1/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/beebe/miniforge3/envs/fcpractice1/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/beebe/miniforge3/envs/fcpractice1/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Training \n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "X_train_aug = cutout_and_rotate(train_x) # 각 X_train 데이터에 대하여 augmentation\n",
    "X_train_aug = X_train_aug.to(device) # 학습을 위하여 GPU에 선언\n",
    "\n",
    "dataset = TensorDataset(train_x, X_train_aug) # augmentation된 데이터와 pair\n",
    "batch_size = 32\n",
    "\n",
    "dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size = batch_size)\n",
    "\n",
    "model = CNN() # 모델 변수 선언\n",
    "loss_func = SimCLR_Loss(batch_size, temperature = 0.5) # loss 함수 선언\n",
    "\n",
    "# train 코드 예시\n",
    "epochs = 10\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for i in range(1, epochs + 1):\n",
    "    total_loss = 0\n",
    "    for data in tqdm(dataloader):\n",
    "        origin_vec = model(data[0])\n",
    "        aug_vec = model(data[1])\n",
    "\n",
    "        loss = loss_func(origin_vec, aug_vec)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch : %d, Avg Loss : %.4f'%(i, total_loss / len(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downstream classification\n",
    "\n",
    "class CNN_classifier(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.CNN = model # contrastive learning으로 학습해둔 모델을 불러오기\n",
    "        self.mlp = nn.Linear(100, 10) # class 차원 개수로 projection\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.CNN(x) # (batch, 100)으로 변환\n",
    "        x = self.mlp(x) # (batch, 10)으로 변환\n",
    "        return x # (batch, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dataset = TensorDataset(X_train, y_train) # 데이터와 라벨 간의 pair\n",
    "batch_size = 32\n",
    "\n",
    "class_dataloader = DataLoader(\n",
    "            class_dataset,\n",
    "            batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_classifier(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.CNN = model # contrastive learning으로 학습해둔 모델을 불러오기\n",
    "        self.mlp = nn.Linear(100, 10) # class 차원 개수로 projection\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.CNN(x) # (batch, 100)으로 변환\n",
    "        x = self.mlp(x) # (batch, 10)으로 변환\n",
    "        return x # (batch, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\beebe\\Desktop\\git_practice\\Practice_DL\\SSL_SimCLR.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/beebe/Desktop/git_practice/Practice_DL/SSL_SimCLR.ipynb#ch0000015?line=0'>1</a>\u001b[0m classifier \u001b[39m=\u001b[39m CNN_classifier(model)\u001b[39m.\u001b[39mto(device) \u001b[39m# 모델 선언, GPU 활용 지정\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/beebe/Desktop/git_practice/Practice_DL/SSL_SimCLR.ipynb#ch0000015?line=2'>3</a>\u001b[0m classifier_loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss() \u001b[39m# 분류를 위한 loss 함수\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/beebe/Desktop/git_practice/Practice_DL/SSL_SimCLR.ipynb#ch0000015?line=4'>5</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = CNN_classifier(model).to(device) # 모델 선언, GPU 활용 지정\n",
    "\n",
    "classifier_loss = nn.CrossEntropyLoss() # 분류를 위한 loss 함수\n",
    "\n",
    "epochs = 10\n",
    "classifier.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n",
    "\n",
    "for i in range(1, epochs + 1):\n",
    "    correct = 0\n",
    "    for data in tqdm(class_dataloader):\n",
    "        logits = classifier(data[0])\n",
    "\n",
    "        loss = classifier_loss(logits, data[1].long())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct += torch.sum(torch.argmax(logits, 1) == data[1]).item() # 정확도 산출을 위하여 정답 개수 누적\n",
    "\n",
    "    print('Epoch : %d, Train Accuracy : %.2f%%'%(i, correct * 100 / len(train_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(test_x, test_y) # 테스트 데이터와 라벨 pair\n",
    "batch_size = 32\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size = batch_size)\n",
    "\n",
    "classifier.eval() # 테스트 모드로 전환\n",
    "\n",
    "correct = 0\n",
    "for data in tqdm(test_dataloader):\n",
    "\n",
    "    logits = classifier(data[0])\n",
    "    correct += torch.sum(torch.argmax(logits, 1) == data[1]).item() # 정확도 산출을 위하여 정답 개수 누적\n",
    "\n",
    "print('Test Accuracy : %.2f%%'%(correct * 100 / len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4cbe91950c8d3c47ce6c9e2bb78622c27789d701432f2f96e459ce65d9fe504"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fcpractice1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
